# üåø LULC-SegNet - Segmenta√ß√£o de Uso e Cobertura do Solo (Petr√≥polis)

Este projeto implementa uma rede neural de **segmenta√ß√£o sem√¢ntica** baseada na arquitetura **SegNet com aten√ß√£o CBAM** para classificar diferentes tipos de uso e cobertura do solo em imagens de sat√©lite do munic√≠pio de Petr√≥polis.

O modelo √© treinado para reconhecer **8 classes** de cobertura do solo:

| Classe | Nome | Cor RGB | Descri√ß√£o |
|--------|------|---------|-----------|
| 0 | Mata Nativa | (0, 100, 0) | Floresta preservada |
| 1 | Vegeta√ß√£o Densa | (0, 255, 0) | √Åreas com densa cobertura vegetal |
| 2 | Ocupa√ß√£o Urbana | (128, 128, 128) | Edifica√ß√µes e infraestrutura urbana |
| 3 | Solo Exposto | (160, 82, 45) | Terrenos sem cobertura vegetal |
| 4 | Corpos d'√Ågua | (0, 0, 255) | Rios, lagos e reservat√≥rios |
| 5 | Agricultura | (255, 255, 0) | √Åreas de cultivo e pastagem |
| 6 | Regenera√ß√£o | (173, 255, 47) | Vegeta√ß√£o em processo de recupera√ß√£o |
| 7 | Sombra | (0, 0, 0) | √Åreas sombreadas |

---

## üìÇ Estrutura do Projeto

```
LULC-SegNet/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ dataset.py          # Dataset customizado e pr√©-processamento
‚îÇ   ‚îú‚îÄ‚îÄ model.py            # Implementa√ß√£o da LULC-SegNet com CBAM
‚îÇ   ‚îú‚îÄ‚îÄ losses.py           # Fun√ß√£o de perda (Focal Loss)
‚îÇ   ‚îú‚îÄ‚îÄ utils.py            # Fun√ß√µes auxiliares (IoU, carregamento de folds)
‚îÇ   ‚îú‚îÄ‚îÄ train.py            # Loop de treinamento e valida√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ main.py             # Script principal para treinamento
‚îÇ   ‚îî‚îÄ‚îÄ inference.py        # Script para infer√™ncia em novas imagens
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ images/             # Imagens RGB de sat√©lite
‚îÇ   ‚îú‚îÄ‚îÄ labels/             # M√°scaras coloridas (ground truth)
‚îÇ   ‚îî‚îÄ‚îÄ folds/              # Divis√µes para valida√ß√£o cruzada
‚îÇ       ‚îú‚îÄ‚îÄ fold1_images.txt
‚îÇ       ‚îú‚îÄ‚îÄ fold1_labels.txt
‚îÇ       ‚îú‚îÄ‚îÄ fold2_images.txt
‚îÇ       ‚îî‚îÄ‚îÄ fold2_labels.txt
‚îÇ
‚îú‚îÄ‚îÄ models/                 # Modelos treinados salvos
‚îÇ   ‚îî‚îÄ‚îÄ lulc_segnet_best.pth
‚îÇ
‚îú‚îÄ‚îÄ results/                # Resultados de infer√™ncia
‚îÇ   ‚îú‚îÄ‚îÄ predictions/        # M√°scaras preditas
‚îÇ   ‚îî‚îÄ‚îÄ visualizations/     # Visualiza√ß√µes coloridas
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt        # Depend√™ncias do projeto
‚îî‚îÄ‚îÄ README.md              # Esta documenta√ß√£o
```

---

## ‚öôÔ∏è Pr√©-requisitos e Instala√ß√£o

### Requisitos de Sistema
- Python **3.9+**
- CUDA 11.0+ (opcional, mas recomendado para GPU)
- 8GB+ RAM
- 4GB+ espa√ßo em disco

### Instala√ß√£o

1. **Clone o reposit√≥rio:**
```bash
git clone https://github.com/seu-usuario/LULC-SegNet.git
cd LULC-SegNet
```

2. **Crie um ambiente virtual:**
```bash
python -m venv lulc_env
source lulc_env/bin/activate  # Linux/Mac
# ou
lulc_env\Scripts\activate     # Windows
```

3. **Instale as depend√™ncias:**
```bash
pip install -r requirements.txt
```

### Depend√™ncias Principais
```txt
torch>=1.12.0
torchvision>=0.13.0
opencv-python>=4.6.0
numpy>=1.21.0
Pillow>=8.3.0
matplotlib>=3.5.0
tqdm>=4.62.0
scikit-learn>=1.0.0
```

---

## üìä Prepara√ß√£o dos Dados

### Formato dos Dados
Organize seus dados seguindo esta estrutura:

```
data/
‚îú‚îÄ‚îÄ images/                 # Imagens RGB (.jpg, .png, .tif)
‚îÇ   ‚îú‚îÄ‚îÄ image_001.jpg
‚îÇ   ‚îú‚îÄ‚îÄ image_002.jpg
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ labels/                 # M√°scaras coloridas (.png)
‚îÇ   ‚îú‚îÄ‚îÄ image_001.png
‚îÇ   ‚îú‚îÄ‚îÄ image_002.png
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ folds/                  # Divis√µes para valida√ß√£o cruzada
    ‚îú‚îÄ‚îÄ fold1_images.txt    # Lista de nomes das imagens para treino
    ‚îú‚îÄ‚îÄ fold1_labels.txt    # Lista de nomes das m√°scaras para treino
    ‚îú‚îÄ‚îÄ fold2_images.txt    # Lista de nomes das imagens para valida√ß√£o
    ‚îî‚îÄ‚îÄ fold2_labels.txt    # Lista de nomes das m√°scaras para valida√ß√£o
```

### Especifica√ß√µes das M√°scaras
- **Formato:** PNG com cores RGB exatas
- **Dimens√µes:** Mesmas das imagens correspondentes
- **Cores:** Devem corresponder exatamente aos valores definidos em `CLASS_COLORS`

### Exemplo de arquivo de fold (fold1_images.txt):
```
image_001.jpg
image_002.jpg
image_003.jpg
```

---

## üöÄ Como Usar

### 1. Treinamento

Para treinar o modelo com os dados preparados:

```bash
cd src/
python main.py
```

**Par√¢metros configur√°veis em `main.py`:**
```python
class Config:
    BATCH_SIZE = 8
    EPOCHS = 100
    LEARNING_RATE = 0.001
    IMG_SIZE = (256, 256)
    NUM_CLASSES = 8
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
```

### 2. Infer√™ncia em Novas Imagens

Para fazer predi√ß√µes em imagens n√£o vistas:

```bash
cd src/
python inference.py --input_path ../data/new_images/ --output_path ../results/ --model_path ../models/lulc_segnet_best.pth
```

**Par√¢metros do script de infer√™ncia:**
- `--input_path`: Caminho para pasta com imagens de entrada
- `--output_path`: Caminho para salvar os resultados
- `--model_path`: Caminho para o modelo treinado
- `--visualize`: Cria visualiza√ß√µes coloridas (opcional)
- `--batch_size`: Tamanho do batch para infer√™ncia (padr√£o: 4)

### 3. Exemplo de Uso da API

```python
from inference import LULCPredictor

# Inicializar o preditor
predictor = LULCPredictor(model_path='../models/lulc_segnet_best.pth')

# Predizer uma √∫nica imagem
mask = predictor.predict_single('path/to/image.jpg')

# Predizer m√∫ltiplas imagens
results = predictor.predict_batch(['img1.jpg', 'img2.jpg'])

# Salvar visualiza√ß√£o
predictor.save_visualization(mask, 'output/prediction.png')
```

---

## üß© Arquitetura do Modelo

### LULC-SegNet com Aten√ß√£o CBAM

A arquitetura combina:

1. **Encoder-Decoder SegNet:** Estrutura base para segmenta√ß√£o
2. **Blocos Residuais:** Melhor propaga√ß√£o de gradientes
3. **CBAM Attention:** Aten√ß√£o em canais e espacial
4. **Skip Connections:** Preserva√ß√£o de detalhes de alta resolu√ß√£o

### Componentes Principais

#### Dataset (`dataset.py`)
- Carregamento e pr√©-processamento de imagens e m√°scaras
- Augmenta√ß√µes: rota√ß√£o, flip horizontal, normaliza√ß√£o
- Convers√£o de m√°scaras coloridas para √≠ndices de classe

#### Modelo (`model.py`)
- **Encoder:** 4 blocos de convolu√ß√£o com pooling
- **Attention:** M√≥dulos CBAM entre encoder e decoder
- **Decoder:** 4 blocos de upsampling com concatena√ß√£o
- **Output:** Camada final com softmax para classifica√ß√£o

#### Fun√ß√£o de Perda (`losses.py`)
- **Focal Loss:** Lida com desbalanceamento entre classes
- Foca em exemplos dif√≠ceis de classificar
- Reduz contribui√ß√£o de exemplos f√°ceis

#### M√©tricas (`utils.py`)
- **IoU (Intersection over Union):** Por classe e m√©dia
- **mIoU:** M√©trica principal de avalia√ß√£o
- **Acur√°cia por pixel**

---

## üìà Monitoramento e Resultados

### Durante o Treinamento
O modelo exibe em tempo real:
- **Loss de treino e valida√ß√£o**
- **mIoU por √©poca**
- **IoU individual por classe**
- **Tempo por √©poca**

### Exemplo de Output:
```
Epoch [15/100] - Train Loss: 0.3456 - Val Loss: 0.4123 - mIoU: 0.7234
Class IoUs: [0.82, 0.79, 0.65, 0.71, 0.89, 0.67, 0.74, 0.52]
Best model saved! (mIoU: 0.7234)
```

### M√©tricas Salvas
- **Modelo com melhor mIoU:** `lulc_segnet_best.pth`
- **Logs de treinamento:** Exibidos no terminal
- **Hist√≥rico:** Pode ser salvo modificando `train.py`

---

## üîß Customiza√ß√£o e Extens√µes

### Adicionando Novas Classes
1. Modifique `CLASS_COLORS` em `main.py`
2. Ajuste `NUM_CLASSES` na configura√ß√£o
3. Prepare m√°scaras com as novas cores
4. Re-treine o modelo

### Modificando Augmenta√ß√µes
Edite a fun√ß√£o `get_transforms()` em `dataset.py`:

```python
def get_transforms(is_training=True):
    if is_training:
        return A.Compose([
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=15, p=0.5),
            A.ColorJitter(brightness=0.2, contrast=0.2, p=0.3),
            A.Resize(height=256, width=256),
            A.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225])
        ])
```

### Ajustando Hiperpar√¢metros
Principais par√¢metros em `main.py`:

```python
class Config:
    BATCH_SIZE = 8          # Ajuste conforme GPU dispon√≠vel
    EPOCHS = 100           # N√∫mero de √©pocas
    LEARNING_RATE = 0.001  # Taxa de aprendizado
    IMG_SIZE = (256, 256)  # Resolu√ß√£o das imagens
    PATIENCE = 10          # Early stopping
```

---

## üìä Resultados Esperados

### Performance T√≠pica
- **mIoU geral:** 0.70-0.85 (dependendo da qualidade dos dados)
- **Classes bem classificadas:** Corpos d'√°gua, Mata Nativa
- **Classes desafiadoras:** Sombra, regenera√ß√£o (devido a similaridades)

### Tempo de Treinamento
- **GPU RTX 3080:** ~30 minutos (100 √©pocas, 1000 imagens)
- **CPU:** ~3-5 horas (mesmo dataset)

### Uso de Mem√≥ria
- **GPU:** 4-6GB VRAM (batch_size=8)
- **RAM:** 8-12GB durante treinamento

---

## üõ†Ô∏è Solu√ß√£o de Problemas

### Problemas Comuns

**1. Erro de CUDA out of memory:**
```bash
# Reduza o batch_size em main.py
BATCH_SIZE = 4  # ou menor
```

**2. M√°scaras com cores incorretas:**
```python
# Verifique se as cores da m√°scara correspondem exatamente a CLASS_COLORS
# Use um editor de imagem para verificar valores RGB
```

**3. Baixa performance:**
- Verifique qualidade das anota√ß√µes
- Aumente augmenta√ß√µes
- Ajuste learning rate
- Considere usar pre-trained weights

**4. Modelo n√£o converge:**
- Verifique se as m√°scaras est√£o corretas
- Reduza learning rate
- Aumente n√∫mero de √©pocas
- Verifique balanceamento das classes

### Debug e Valida√ß√£o

Para verificar se os dados est√£o corretos:

```python
# Adicione em dataset.py para debug
import matplotlib.pyplot as plt

def visualize_sample(dataset, idx):
    image, mask = dataset[idx]
    plt.figure(figsize=(12, 4))
    plt.subplot(131)
    plt.imshow(image.permute(1, 2, 0))
    plt.title('Original')
    plt.subplot(132)
    plt.imshow(mask, cmap='tab10')
    plt.title('Ground Truth')
    plt.show()
```

---

## üí° Pr√≥ximos Desenvolvimentos

### Funcionalidades Planejadas
- [ ] **Valida√ß√£o cruzada k-fold** automatizada
- [ ] **Visualiza√ß√£o interativa** dos resultados
- [ ] **Exporta√ß√£o ONNX/TorchScript** para deploy
- [ ] **API REST** para infer√™ncia online
- [ ] **Docker container** para facilitar deployment
- [ ] **M√©tricas adicionais:** Precis√£o, Recall, F1-Score por classe
- [ ] **Ensemble de modelos** para melhor performance
- [ ] **Data augmentation avan√ßada** com t√©cnicas espec√≠ficas para sensoriamento remoto

### Melhorias T√©cnicas
- [ ] **Mixed precision training** para acelerar treinamento
- [ ] **Learning rate scheduling** adaptativo
- [ ] **TensorBoard logging** para melhor monitoramento
- [ ] **Checkpointing autom√°tico** a cada √©poca
- [ ] **Multi-GPU support** para datasets maiores

---

## üìö Refer√™ncias e Inspira√ß√µes

### Papers Relevantes
- **SegNet:** Badrinarayanan, V., et al. "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation."
- **CBAM:** Woo, S., et al. "CBAM: Convolutional Block Attention Module."
- **Focal Loss:** Lin, T.Y., et al. "Focal Loss for Dense Object Detection."

### Datasets Similares
- **ISPRS Potsdam:** Rottensteiner, F., et al.
- **INRIA Aerial Image Dataset**
- **Massachusetts Buildings Dataset**

---

## ü§ù Contribui√ß√£o

### Como Contribuir
1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### Diretrizes
- Mantenha o c√≥digo documentado
- Adicione testes para novas funcionalidades
- Siga as conven√ß√µes de nomenclatura existentes
- Atualize a documenta√ß√£o quando necess√°rio

---

## üìú Licen√ßa

Este projeto foi desenvolvido como **Trabalho de Conclus√£o de Curso** para fins acad√™micos. 

**Uso Acad√™mico:** Livre para pesquisa e educa√ß√£o  
**Uso Comercial:** Entre em contato com os autores

---

## üë®‚Äçüíª Autores e Contato

**Desenvolvido por:** [Seu Nome]  
**Orientador:** [Nome do Orientador]  
**Institui√ß√£o:** [Sua Universidade]  

**Contatos:**
- üìß Email: seu.email@universidade.edu.br
- üì± LinkedIn: [seu-perfil]
- üêô GitHub: [seu-usuario]

---

## üôè Agradecimentos

Agradecimentos especiais √†:
- **Prefeitura de Petr√≥polis** pelo fornecimento dos dados
- **Laborat√≥rio de Sensoriamento Remoto** pela infraestrutura
- **Comunidade PyTorch** pelas ferramentas excelentes
- **Reviewers e colegas** pelas sugest√µes valiosas

---

### üìã Checklist de Implementa√ß√£o

- [x] Arquitetura SegNet com CBAM
- [x] Dataset customizado para LULC
- [x] Focal Loss para classes desbalanceadas
- [x] Script de treinamento completo
- [x] Script de infer√™ncia
- [x] Documenta√ß√£o abrangente
- [ ] Valida√ß√£o cruzada k-fold
- [ ] Interface web para demonstra√ß√£o
- [ ] Containeriza√ß√£o Docker
- [ ] CI/CD pipeline

---

**‚≠ê Se este projeto foi √∫til para voc√™, considere dar uma estrela no GitHub!**

---

*Este README foi criado com ‚ù§Ô∏è para facilitar o uso e desenvolvimento do LULC-SegNet.*