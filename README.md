# ğŸŒ¿ LULC-SegNet - SegmentaÃ§Ã£o de Uso e Cobertura do Solo (PetrÃ³polis)

Este projeto implementa uma rede neural de **segmentaÃ§Ã£o semÃ¢ntica** baseada na arquitetura **SegNet com atenÃ§Ã£o CBAM** para classificar diferentes tipos de uso e cobertura do solo em imagens de satÃ©lite do municÃ­pio de PetrÃ³polis.

O modelo Ã© treinado para reconhecer **8 classes** de cobertura do solo:

| Classe | Nome | Cor RGB | DescriÃ§Ã£o |
|--------|------|---------|-----------|
| 0 | Mata Nativa | (0, 100, 0) | Floresta preservada |
| 1 | VegetaÃ§Ã£o Densa | (0, 255, 0) | Ãreas com densa cobertura vegetal |
| 2 | OcupaÃ§Ã£o Urbana | (128, 128, 128) | EdificaÃ§Ãµes e infraestrutura urbana |
| 3 | Solo Exposto | (160, 82, 45) | Terrenos sem cobertura vegetal |
| 4 | Corpos d'Ãgua | (0, 0, 255) | Rios, lagos e reservatÃ³rios |
| 5 | Agricultura | (255, 255, 0) | Ãreas de cultivo e pastagem |
| 6 | RegeneraÃ§Ã£o | (173, 255, 47) | VegetaÃ§Ã£o em processo de recuperaÃ§Ã£o |
| 7 | Sombra | (0, 0, 0) | Ãreas sombreadas |

---

## ğŸ“‚ Estrutura do Projeto

```
LULC-SegNet/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py          # Dataset customizado e prÃ©-processamento
â”‚   â”œâ”€â”€ model.py            # ImplementaÃ§Ã£o da LULC-SegNet com CBAM
â”‚   â”œâ”€â”€ losses.py           # FunÃ§Ã£o de perda (Focal Loss)
â”‚   â”œâ”€â”€ utils.py            # FunÃ§Ãµes auxiliares (IoU, carregamento de folds)
â”‚   â”œâ”€â”€ train.py            # Loop de treinamento e validaÃ§Ã£o
â”‚   â”œâ”€â”€ main.py             # Script principal para treinamento
â”‚   â””â”€â”€ inference.py        # Script para inferÃªncia em novas imagens
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ images/             # Imagens RGB de satÃ©lite
â”‚   â”œâ”€â”€ labels/             # MÃ¡scaras coloridas (ground truth)
â”‚   â””â”€â”€ folds/              # DivisÃµes para validaÃ§Ã£o cruzada
â”‚       â”œâ”€â”€ fold1_images.txt
â”‚       â”œâ”€â”€ fold1_labels.txt
â”‚       â”œâ”€â”€ fold2_images.txt
â”‚       â””â”€â”€ fold2_labels.txt
â”‚
â”œâ”€â”€ models/                 # Modelos treinados salvos
â”‚   â””â”€â”€ lulc_segnet_best.pth
â”‚
â”œâ”€â”€ results/                # Resultados de inferÃªncia
â”‚   â”œâ”€â”€ predictions/        # MÃ¡scaras preditas
â”‚   â””â”€â”€ visualizations/     # VisualizaÃ§Ãµes coloridas
â”‚
â”œâ”€â”€ requirements.txt        # DependÃªncias do projeto
â””â”€â”€ README.md              # Esta documentaÃ§Ã£o
```

---

## âš™ï¸ PrÃ©-requisitos e InstalaÃ§Ã£o

### Requisitos de Sistema
- Python **3.9+**
- CUDA 11.0+ (opcional, mas recomendado para GPU)
- 8GB+ RAM
- 4GB+ espaÃ§o em disco

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**
```bash
git clone https://github.com/seu-usuario/LULC-SegNet.git
cd LULC-SegNet
```

2. **Crie um ambiente virtual:**
```bash
python -m venv lulc_env
source lulc_env/bin/activate  # Linux/Mac
# ou
lulc_env\Scripts\activate     # Windows
```

3. **Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

### DependÃªncias Principais
```txt
torch>=1.12.0
torchvision>=0.13.0
opencv-python>=4.6.0
numpy>=1.21.0
Pillow>=8.3.0
matplotlib>=3.5.0
tqdm>=4.62.0
scikit-learn>=1.0.0
```

---

## ğŸ“Š PreparaÃ§Ã£o dos Dados

### Formato dos Dados
Organize seus dados seguindo esta estrutura:

```
data/
â”œâ”€â”€ images/                 # Imagens RGB (.jpg, .png, .tif)
â”‚   â”œâ”€â”€ image_001.jpg
â”‚   â”œâ”€â”€ image_002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ labels/                 # MÃ¡scaras coloridas (.png)
â”‚   â”œâ”€â”€ image_001.png
â”‚   â”œâ”€â”€ image_002.png
â”‚   â””â”€â”€ ...
â””â”€â”€ folds/                  # DivisÃµes para validaÃ§Ã£o cruzada
    â”œâ”€â”€ fold1_images.txt    # Lista de nomes das imagens para treino
    â”œâ”€â”€ fold1_labels.txt    # Lista de nomes das mÃ¡scaras para treino
    â”œâ”€â”€ fold2_images.txt    # Lista de nomes das imagens para validaÃ§Ã£o
    â””â”€â”€ fold2_labels.txt    # Lista de nomes das mÃ¡scaras para validaÃ§Ã£o
```

### EspecificaÃ§Ãµes das MÃ¡scaras
- **Formato:** PNG com cores RGB exatas
- **DimensÃµes:** Mesmas das imagens correspondentes
- **Cores:** Devem corresponder exatamente aos valores definidos em `CLASS_COLORS`

### Exemplo de arquivo de fold (fold1_images.txt):
```
image_001.jpg
image_002.jpg
image_003.jpg
```

---

## ğŸš€ Como Usar

### 1. Treinamento

Para treinar o modelo com os dados preparados:

```bash
cd src/
python main.py
```

**ParÃ¢metros configurÃ¡veis em `main.py`:**
```python
class Config:
    BATCH_SIZE = 8
    EPOCHS = 100
    LEARNING_RATE = 0.001
    IMG_SIZE = (256, 256)
    NUM_CLASSES = 8
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
```

### 2. InferÃªncia em Novas Imagens

Para fazer prediÃ§Ãµes em imagens nÃ£o vistas:

```bash
cd src/
python inference.py --input_path ../data/new_images/ --output_path ../results/ --model_path ../models/lulc_segnet_best.pth
```

**ParÃ¢metros do script de inferÃªncia:**
- `--input_path`: Caminho para pasta com imagens de entrada
- `--output_path`: Caminho para salvar os resultados
- `--model_path`: Caminho para o modelo treinado
- `--visualize`: Cria visualizaÃ§Ãµes coloridas (opcional)
- `--batch_size`: Tamanho do batch para inferÃªncia (padrÃ£o: 4)

### 3. Exemplo de Uso da API

```python
from inference import LULCPredictor

# Inicializar o preditor
predictor = LULCPredictor(model_path='../models/lulc_segnet_best.pth')

# Predizer uma Ãºnica imagem
mask = predictor.predict_single('path/to/image.jpg')

# Predizer mÃºltiplas imagens
results = predictor.predict_batch(['img1.jpg', 'img2.jpg'])

# Salvar visualizaÃ§Ã£o
predictor.save_visualization(mask, 'output/prediction.png')
```

---

## ğŸ§© Arquitetura do Modelo

### LULC-SegNet com AtenÃ§Ã£o CBAM

A arquitetura combina:

1. **Encoder-Decoder SegNet:** Estrutura base para segmentaÃ§Ã£o
2. **Blocos Residuais:** Melhor propagaÃ§Ã£o de gradientes
3. **CBAM Attention:** AtenÃ§Ã£o em canais e espacial
4. **Skip Connections:** PreservaÃ§Ã£o de detalhes de alta resoluÃ§Ã£o

### Componentes Principais

#### Dataset (`dataset.py`)
- Carregamento e prÃ©-processamento de imagens e mÃ¡scaras
- AugmentaÃ§Ãµes: rotaÃ§Ã£o, flip horizontal, normalizaÃ§Ã£o
- ConversÃ£o de mÃ¡scaras coloridas para Ã­ndices de classe

#### Modelo (`model.py`)
- **Encoder:** 4 blocos de convoluÃ§Ã£o com pooling
- **Attention:** MÃ³dulos CBAM entre encoder e decoder
- **Decoder:** 4 blocos de upsampling com concatenaÃ§Ã£o
- **Output:** Camada final com softmax para classificaÃ§Ã£o

#### FunÃ§Ã£o de Perda (`losses.py`)
- **Focal Loss:** Lida com desbalanceamento entre classes
- Foca em exemplos difÃ­ceis de classificar
- Reduz contribuiÃ§Ã£o de exemplos fÃ¡ceis

#### MÃ©tricas (`utils.py`)
- **IoU (Intersection over Union):** Por classe e mÃ©dia
- **mIoU:** MÃ©trica principal de avaliaÃ§Ã£o
- **AcurÃ¡cia por pixel**

---

## ğŸ“ˆ Monitoramento e Resultados

### Durante o Treinamento
O modelo exibe em tempo real:
- **Loss de treino e validaÃ§Ã£o**
- **mIoU por Ã©poca**
- **IoU individual por classe**
- **Tempo por Ã©poca**

### Exemplo de Output:
```
Epoch [15/100] - Train Loss: 0.3456 - Val Loss: 0.4123 - mIoU: 0.7234
Class IoUs: [0.82, 0.79, 0.65, 0.71, 0.89, 0.67, 0.74, 0.52]
Best model saved! (mIoU: 0.7234)
```

### MÃ©tricas Salvas
- **Modelo com melhor mIoU:** `lulc_segnet_best.pth`
- **Logs de treinamento:** Exibidos no terminal
- **HistÃ³rico:** Pode ser salvo modificando `train.py`

---

## ğŸ”§ CustomizaÃ§Ã£o e ExtensÃµes

### Adicionando Novas Classes
1. Modifique `CLASS_COLORS` em `main.py`
2. Ajuste `NUM_CLASSES` na configuraÃ§Ã£o
3. Prepare mÃ¡scaras com as novas cores
4. Re-treine o modelo

### Modificando AugmentaÃ§Ãµes
Edite a funÃ§Ã£o `get_transforms()` em `dataset.py`:

```python
def get_transforms(is_training=True):
    if is_training:
        return A.Compose([
            A.HorizontalFlip(p=0.5),
            A.Rotate(limit=15, p=0.5),
            A.ColorJitter(brightness=0.2, contrast=0.2, p=0.3),
            A.Resize(height=256, width=256),
            A.Normalize(mean=[0.485, 0.456, 0.406], 
                       std=[0.229, 0.224, 0.225])
        ])
```

### Ajustando HiperparÃ¢metros
Principais parÃ¢metros em `main.py`:

```python
class Config:
    BATCH_SIZE = 8          # Ajuste conforme GPU disponÃ­vel
    EPOCHS = 100           # NÃºmero de Ã©pocas
    LEARNING_RATE = 0.001  # Taxa de aprendizado
    IMG_SIZE = (256, 256)  # ResoluÃ§Ã£o das imagens
    PATIENCE = 10          # Early stopping
```

---

## ğŸ“Š Resultados Esperados

### Performance TÃ­pica
- **mIoU geral:** 0.70-0.85 (dependendo da qualidade dos dados)
- **Classes bem classificadas:** Corpos d'Ã¡gua, Mata Nativa
- **Classes desafiadoras:** Sombra, regeneraÃ§Ã£o (devido a similaridades)

### Tempo de Treinamento
- **GPU RTX 3080:** ~30 minutos (100 Ã©pocas, 1000 imagens)
- **CPU:** ~3-5 horas (mesmo dataset)

### Uso de MemÃ³ria
- **GPU:** 4-6GB VRAM (batch_size=8)
- **RAM:** 8-12GB durante treinamento

---

## ğŸ› ï¸ SoluÃ§Ã£o de Problemas

### Problemas Comuns

**1. Erro de CUDA out of memory:**
```bash
# Reduza o batch_size em main.py
BATCH_SIZE = 4  # ou menor
```

**2. MÃ¡scaras com cores incorretas:**
```python
# Verifique se as cores da mÃ¡scara correspondem exatamente a CLASS_COLORS
# Use um editor de imagem para verificar valores RGB
```

**3. Baixa performance:**
- Verifique qualidade das anotaÃ§Ãµes
- Aumente augmentaÃ§Ãµes
- Ajuste learning rate
- Considere usar pre-trained weights

**4. Modelo nÃ£o converge:**
- Verifique se as mÃ¡scaras estÃ£o corretas
- Reduza learning rate
- Aumente nÃºmero de Ã©pocas
- Verifique balanceamento das classes

### Debug e ValidaÃ§Ã£o

Para verificar se os dados estÃ£o corretos:

```python
# Adicione em dataset.py para debug
import matplotlib.pyplot as plt

def visualize_sample(dataset, idx):
    image, mask = dataset[idx]
    plt.figure(figsize=(12, 4))
    plt.subplot(131)
    plt.imshow(image.permute(1, 2, 0))
    plt.title('Original')
    plt.subplot(132)
    plt.imshow(mask, cmap='tab10')
    plt.title('Ground Truth')
    plt.show()
```

---

## ğŸ’¡ PrÃ³ximos Desenvolvimentos

### Funcionalidades Planejadas
- [ ] **ValidaÃ§Ã£o cruzada k-fold** automatizada
- [ ] **VisualizaÃ§Ã£o interativa** dos resultados
- [ ] **ExportaÃ§Ã£o ONNX/TorchScript** para deploy
- [ ] **API REST** para inferÃªncia online
- [ ] **Docker container** para facilitar deployment
- [ ] **MÃ©tricas adicionais:** PrecisÃ£o, Recall, F1-Score por classe
- [ ] **Ensemble de modelos** para melhor performance
- [ ] **Data augmentation avanÃ§ada** com tÃ©cnicas especÃ­ficas para sensoriamento remoto

### Melhorias TÃ©cnicas
- [ ] **Mixed precision training** para acelerar treinamento
- [ ] **Learning rate scheduling** adaptativo
- [ ] **TensorBoard logging** para melhor monitoramento
- [ ] **Checkpointing automÃ¡tico** a cada Ã©poca
- [ ] **Multi-GPU support** para datasets maiores

---

## ğŸ“š ReferÃªncias e InspiraÃ§Ãµes

### Papers Relevantes
- **SegNet:** Badrinarayanan, V., et al. "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation."
- **CBAM:** Woo, S., et al. "CBAM: Convolutional Block Attention Module."
- **Focal Loss:** Lin, T.Y., et al. "Focal Loss for Dense Object Detection."

### Datasets Similares
- **ISPRS Potsdam:** Rottensteiner, F., et al.
- **INRIA Aerial Image Dataset**
- **Massachusetts Buildings Dataset**

---

## ğŸ¤ ContribuiÃ§Ã£o

### Como Contribuir
1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

### Diretrizes
- Mantenha o cÃ³digo documentado
- Adicione testes para novas funcionalidades
- Siga as convenÃ§Ãµes de nomenclatura existentes
- Atualize a documentaÃ§Ã£o quando necessÃ¡rio

---

## ğŸ“œ LicenÃ§a

Este projeto foi desenvolvido como **Trabalho de ConclusÃ£o de Curso** para fins acadÃªmicos. 

**Uso AcadÃªmico:** Livre para pesquisa e educaÃ§Ã£o  
**Uso Comercial:** Entre em contato com os autores

---

## ğŸ‘¨â€ğŸ’» Autores e Contato

**Desenvolvido por:** [Seu Nome]  
**Orientador:** [Nome do Orientador]  
**InstituiÃ§Ã£o:** [Sua Universidade]  

**Contatos:**
- ğŸ“§ Email: seu.email@universidade.edu.br
- ğŸ“± LinkedIn: [seu-perfil]
- ğŸ™ GitHub: [seu-usuario]

---

## ğŸ™ Agradecimentos

Agradecimentos especiais Ã :
- **Prefeitura de PetrÃ³polis** pelo fornecimento dos dados
- **LaboratÃ³rio de Sensoriamento Remoto** pela infraestrutura
- **Comunidade PyTorch** pelas ferramentas excelentes
- **Reviewers e colegas** pelas sugestÃµes valiosas

---

### ğŸ“‹ Checklist de ImplementaÃ§Ã£o

- [x] Arquitetura SegNet com CBAM
- [x] Dataset customizado para LULC
- [x] Focal Loss para classes desbalanceadas
- [x] Script de treinamento completo
- [x] Script de inferÃªncia
- [x] DocumentaÃ§Ã£o abrangente
- [ ] ValidaÃ§Ã£o cruzada k-fold
- [ ] Interface web para demonstraÃ§Ã£o
- [ ] ContainerizaÃ§Ã£o Docker
- [ ] CI/CD pipeline

---

**â­ Se este projeto foi Ãºtil para vocÃª, considere dar uma estrela no GitHub!**

---

*Este README foi criado com â¤ï¸ para facilitar o uso e desenvolvimento do LULC-SegNet.*